{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rM72L1Vooj_",
    "outputId": "45c3003b-6794-44ac-d9dc-27b3a6cb598c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A2b3uNikqyS8",
    "outputId": "ea966096-48a5-4ab6-fc3c-5941a6a47b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/49/a812ed93088ba9519cbb40eb9f52341694b31cfa126bfddcd9db3761f3ac/flair-0.6.1.post1-py3-none-any.whl (337kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
      "Collecting janome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7MB 49.4MB/s \n",
      "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/76/a1/05d7f62f956d77b23a640efc650f80ce24483aa2f85a09c03fb64f49e879/Deprecated-1.2.10-py2.py3-none-any.whl\n",
      "Collecting mpld3==0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 37.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.7.0+cu101)\n",
      "Collecting transformers>=3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/4e/4f1ede0fd7a36278844a277f8d53c21f88f37f3754abf76a5d6224f76d4a/transformers-3.4.0-py3-none-any.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 37.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
      "\u001b[K     |████████████████████████████████| 983kB 37.9MB/s \n",
      "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1MB 37.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
      "Collecting ftfy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 7.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->flair) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.18.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.3.2->flair) (2.23.0)\n",
      "Collecting overrides==3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (0.7)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->flair) (3.7.4.3)\n",
      "Collecting tokenizers==0.9.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a5/78be1a55b2ac8d6a956f0a211d372726e2b1dd2666bb537fea9b03abd62c/tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 35.5MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 34.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.12.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (20.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.17.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (3.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bpemb>=0.3.2->flair) (3.0.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers>=3.0.0->flair) (50.3.2)\n",
      "Building wheels for collected packages: mpld3, langdetect, sqlitedict, ftfy, segtok, overrides, sacremoses\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116677 sha256=216a2bfd2999019cc8f60ab73502213e1e01558fc0d7ce91a46677de647b7179\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993195 sha256=ffade0bf58b6783180a25bb99b2d6b64b7c7f0a30c646c0781d57a5bfcb77380\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14377 sha256=000794069c9d0076becc6be8ee71a24b7a6aa54d6a074b04e9b952627f4b6545\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45612 sha256=e5ffb213dcf800cff2d83b38c9bfa3b973660ed345888b7e639aba7180ff74e3\n",
      "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25021 sha256=a4dd78d0b6136bcd5ce081e12481d0fbef49e434cdce6895332aee0ff09c3402\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=81073117831a7312464968107d075344fc9b4e0b43512776d9017e307091e5ed\n",
      "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=4d8078417cc2a5e758d28b58428a59547f9316629f748334df7cc84cccbcf28f\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built mpld3 langdetect sqlitedict ftfy segtok overrides sacremoses\n",
      "Installing collected packages: sentencepiece, bpemb, janome, overrides, konoha, deprecated, mpld3, tokenizers, sacremoses, transformers, langdetect, sqlitedict, ftfy, segtok, flair\n",
      "Successfully installed bpemb-0.3.2 deprecated-1.2.10 flair-0.6.1.post1 ftfy-5.8 janome-0.4.1 konoha-4.6.2 langdetect-1.0.8 mpld3-0.3 overrides-3.0.0 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.94 sqlitedict-1.7.0 tokenizers-0.9.2 transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xd1nkBxt1HEe"
   },
   "outputs": [],
   "source": [
    "#important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "#importing stopwords is optional, in this case it decreased accuracy\n",
    "#from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "R_kP8hWwkI5J",
    "outputId": "848dcd9b-dc49-45a9-bdad-bf9607358cee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7635</th>\n",
       "      <td>worry</td>\n",
       "      <td>@ajlnike09 omg wtf why is there a porn site fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21844</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Surf's up this week - starting this afternoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>sadness</td>\n",
       "      <td>We're not gonna be able to talk again</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26372</th>\n",
       "      <td>happiness</td>\n",
       "      <td>this is my second to last biology and the firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15029</th>\n",
       "      <td>sadness</td>\n",
       "      <td>@theAV8TR dont quit.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment                                            content\n",
       "7635       worry  @ajlnike09 omg wtf why is there a porn site fo...\n",
       "21844    neutral      Surf's up this week - starting this afternoon\n",
       "7542     sadness              We're not gonna be able to talk again\n",
       "26372  happiness  this is my second to last biology and the firs...\n",
       "15029    sadness                               @theAV8TR dont quit."
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "import pandas as pd\n",
    "\n",
    "#col_names = ['sentiment','id','date','query_string','user','text']\n",
    "data_path = '/content/drive/My Drive/Twitter data/text_emotion.csv'\n",
    "\n",
    "\n",
    "tweet_data = pd.read_csv(data_path, encoding=\"ISO-8859-1\").sample(frac=1) # .sample(frac=1) shuffles the data\n",
    "tweet_data = tweet_data[['sentiment', 'content']] # Disregard other columns\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0TjPoZ59HqVx"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/content/drive/My Drive/Twitter data/archive/train.txt\", delimiter=';', header=None, names=['sentence','label'])\n",
    "df_test = pd.read_csv(\"/content/drive/My Drive/Twitter data/archive/test.txt\", delimiter=';', header=None, names=['sentence','label'])\n",
    "df_val = pd.read_csv(\"/content/drive/My Drive/Twitter data/archive/val.txt\", delimiter=';', header=None, names=['sentence','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDYuyWHLHvy_",
    "outputId": "4e3ef03f-a25c-495a-a449-d25aacaf2a14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'anger', 'love', 'surprise', 'fear', 'joy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([df_train,df_test,df_val])\n",
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "TRncSFByJeH6",
    "outputId": "133f596d-661f-47b0-b0cb-51c00f3b7df7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ive been taking or milligrams or times recomme...</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i feel as confused about life as a teenager or...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i feel romantic too</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label\n",
       "0                            i didnt feel humiliated   sadness\n",
       "1  i can go from feeling so hopeless to so damned...   sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong     anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      love\n",
       "4                               i am feeling grouchy     anger\n",
       "5  ive been feeling a little burdened lately wasn...   sadness\n",
       "6  ive been taking or milligrams or times recomme...  surprise\n",
       "7  i feel as confused about life as a teenager or...      fear\n",
       "8  i have been with petronas for years i feel tha...       joy\n",
       "9                                i feel romantic too      love"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4EWiJjE3JCeo"
   },
   "outputs": [],
   "source": [
    "#change column label to sentiment\n",
    "df = df.rename(columns = {\"label\":\"sentiment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hg_ryAvIaMH",
    "outputId": "7e898e0f-885b-4d3d-82b4-9794869db5e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of data based on labels:  joy         6761\n",
      "sadness     5797\n",
      "anger       2709\n",
      "fear        2373\n",
      "love        1641\n",
      "surprise     719\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#check distribution of data based on labels\n",
    "print(\"Distribution of data based on labels: \",df.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ohOUrMH1H6ni"
   },
   "outputs": [],
   "source": [
    "#tweet_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6i4Z6n6JlLT"
   },
   "outputs": [],
   "source": [
    "#change column content to sentence\n",
    "#tweet_data = tweet_data.rename(columns = {\"content\":\"sentence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upv5K3Y_IK92"
   },
   "outputs": [],
   "source": [
    "#check distribution of data based on labels\n",
    "#print(\"Distribution of data based on labels: \",tweet_data.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFe9PdDpJXoV"
   },
   "outputs": [],
   "source": [
    "# merge both df and tweet_data\n",
    "# final_df = pd.concat([df, tweet_data])\n",
    "# len(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hq8Y4A5DKMrd"
   },
   "outputs": [],
   "source": [
    "#final_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxnloyJrIjqb"
   },
   "outputs": [],
   "source": [
    "#remove certain categories from the data: fun, relief, \n",
    "#combine both hate and anger\n",
    "# combine happiness with enthusiasm and joy\n",
    "# combine empty with sadness\n",
    "\n",
    "#final_df['sentiment'] = final_df['sentiment'].replace({\"joy\":\"happiness\", \"hate\":\"anger\", \"enthusiasm\":\"happiness\", \"hate\":\"anger\", \"empty\":\"sadness\", \"boredom\":\"sadness\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ve3N-lgOK9pi"
   },
   "outputs": [],
   "source": [
    "#final_df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3KApE8vwkI5U"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "allowed_chars = ' AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz0123456789~`!@#$%^&*()-=_+[]{}|;:\",./<>?'\n",
    "punct = '!?,.@#'\n",
    "maxlen = 280\n",
    "\n",
    "def preprocess(text):\n",
    "    return ''.join([' ' + char + ' ' if char in punct else char for char in [char for char in re.sub(r'http\\S+', 'http', text, flags=re.MULTILINE) if char in allowed_chars]])[:maxlen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "B6gCJMr_kI5d"
   },
   "outputs": [],
   "source": [
    "df['sentence'] = df['sentence'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOejQ7iJWVE7"
   },
   "outputs": [],
   "source": [
    "#final_df.to_csv(data_dir + '/final.csv', header= True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jh2j45uWfiw"
   },
   "outputs": [],
   "source": [
    "#data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7gXTLsx12WTC"
   },
   "outputs": [],
   "source": [
    "# change the categories of sentiment into numerical form\n",
    "df[\"sentiment\"] = df[\"sentiment\"].astype('category')\n",
    "df[\"sentiment\"] = df[\"sentiment\"].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NgwJYlLCkI5i"
   },
   "outputs": [],
   "source": [
    "df['sentiment'] = '__label__' + df['sentiment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "tDg9ep0SkI5n",
    "outputId": "455f407b-a978-40e9-9e57-4350e44dbade"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>__label__4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>__label__4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>__label__3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ive been feeling a little burdened lately wasn...</td>\n",
       "      <td>__label__4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ive been taking or milligrams or times recomme...</td>\n",
       "      <td>__label__5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i feel as confused about life as a teenager or...</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i have been with petronas for years i feel tha...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i feel romantic too</td>\n",
       "      <td>__label__3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence   sentiment\n",
       "0                            i didnt feel humiliated  __label__4\n",
       "1  i can go from feeling so hopeless to so damned...  __label__4\n",
       "2   im grabbing a minute to post i feel greedy wrong  __label__0\n",
       "3  i am ever feeling nostalgic about the fireplac...  __label__3\n",
       "4                               i am feeling grouchy  __label__0\n",
       "5  ive been feeling a little burdened lately wasn...  __label__4\n",
       "6  ive been taking or milligrams or times recomme...  __label__5\n",
       "7  i feel as confused about life as a teenager or...  __label__1\n",
       "8  i have been with petronas for years i feel tha...  __label__2\n",
       "9                                i feel romantic too  __label__3"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rhkFgVaOPq__"
   },
   "outputs": [],
   "source": [
    "cols = [\"sentiment\", \"sentence\"]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rbAgwmQmkI5t"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory for saving data if it does not already exist\n",
    "data_dir = '/content/drive/My Drive/Twitter data/emotions/new_data/'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# Save a percentage of the data (you could also only load a fraction of the data instead)\n",
    "\n",
    "df.iloc[0:int(len(df)*0.8)].to_csv(data_dir + '/train.csv', index=False, header=False)\n",
    "df.iloc[int(len(df)*0.8):int(len(df)*0.9)].to_csv(data_dir + '/test.csv', index=False, header=False)\n",
    "df.iloc[int(len(df)*0.9):int(len(df)*1.0)].to_csv(data_dir + '/dev.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44nlgzX-kI5y",
    "outputId": "98290024-e249-42ad-920f-51806744aa16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:01:22,378 Reading data from /content/drive/My Drive/Twitter data/emotions/new_data\n",
      "2020-11-09 06:01:22,379 Train: /content/drive/My Drive/Twitter data/emotions/new_data/train.csv\n",
      "2020-11-09 06:01:22,385 Dev: /content/drive/My Drive/Twitter data/emotions/new_data/dev.csv\n",
      "2020-11-09 06:01:22,387 Test: /content/drive/My Drive/Twitter data/emotions/new_data/test.csv\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import ClassificationCorpus\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = data_dir\n",
    "\n",
    "# load corpus containing training, test and dev data\n",
    "corpus: Corpus = ClassificationCorpus(data_folder,\n",
    "                                      test_file='test.csv',\n",
    "                                      dev_file='dev.csv',\n",
    "                                      train_file='train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "AiYkJTx1kI52"
   },
   "outputs": [],
   "source": [
    "#from flair.data_fetcher import NLPTaskDataFettcher\n",
    "#from pathlib import Path\n",
    "\n",
    "#corpus = NLPTaskDataFetcher.load_classification_corpus(Path(data_dir), test_file='test.csv', dev_file='dev.csv', train_file='train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O1vrZLsvkI55",
    "outputId": "5c090f60-8194-4122-8052-68ee2b285e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:01:22,636 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18000/18000 [00:15<00:00, 1146.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:01:38,500 [b'4,i', b'0,im', b'3,i', b'0,i', b'4,ive', b'5,ive', b'1,i', b'2,i', b'5,i', b'2,ive', b'2,on', b'4,im', b'4,when', b'2,im', b'2,finding', b'5,im', b'1,im', b'1,when', b'0,id', b'4,id', b'1,fear', b'2,ill', b'2,id', b'3,im', b'1,ive', b'3,is', b'4,the', b'0,when', b'0,ive', b'2,when', b'0,my', b'1,ill', b'0,this', b'2,my', b'1,as', b'4,my', b'1,waiting', b'3,id', b'0,a', b'2,imdoing', b'1,while', b'4,a', b'3,ive', b'0,we', b'2,is', b'4,ill', b'0,sometime', b'0,ill', b'2,there', b'0,in', b'1,stranded', b'4,after', b'1,earth', b'4,for', b'0,the', b'2,during', b'0,at', b'2,arriving', b'1,staying', b'2,the', b'0,getting', b'1,during', b'2,a', b'1,watched', b'4,death', b'4,during', b'2,having', b'1,tutorial', b'0,heated', b'1,in', b'1,once', b'2,went', b'0,first', b'1,going', b'3,ill', b'1,every', b'1,my', b'0,during', b'0,watching', b'4,this', b'2,boy', b'0,no', b'1,occured', b'1,id', b'0,having', b'0,being', b'2,iv', b'0,realizing', b'1,the', b'5,ill', b'1,on', b'4,hearing', b'2,one', b'2,being', b'4,one', b'2,this', b'0,there', b'1,before', b'1,finding', b'0,one', b'5,id', b'0,as', b'0,insulted', b'0,listening', b'0,is', b'2,day', b'0,four', b'0,out', b'4,lost', b'0,discovering', b'2,made', b'0,told', b'2,getting', b'0,whenever', b'2,always']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_dict = corpus.make_label_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taW_b8eikI5-",
    "outputId": "7fb3a58c-281a-4e90-fc62-eafae9ddc6f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:01:39,130 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpwmpk1xe8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 160000128/160000128 [00:07<00:00, 22456670.81B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:01:46,606 copying /tmp/tmpwmpk1xe8 to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:01:46,930 removing temp file /tmp/tmpwmpk1xe8\n",
      "2020-11-09 06:01:47,318 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmpij_4vtlo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21494764/21494764 [00:01<00:00, 12172654.77B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:01:49,436 copying /tmp/tmpij_4vtlo to cache at /root/.flair/embeddings/glove.gensim\n",
      "2020-11-09 06:01:49,467 removing temp file /tmp/tmpij_4vtlo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings\n",
    "\n",
    "word_embeddings = [WordEmbeddings('glove'),\n",
    "                    #FlairEmbeddings('news-forward'),\n",
    "                    #FlairEmbeddings('news-backward')\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Yg2m-98hkI6C"
   },
   "outputs": [],
   "source": [
    "from flair.embeddings import DocumentRNNEmbeddings\n",
    "\n",
    "document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "XG_UF56MkI6G"
   },
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aBrAFfOUkI6L"
   },
   "outputs": [],
   "source": [
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wW31tGxwkI6Q",
    "outputId": "d0d744bc-825c-48af-818a-9dde9700e165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-06 11:58:01,246 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 11:58:01,249 Model: \"TextClassifier(\n",
      "  (document_embeddings): DocumentRNNEmbeddings(\n",
      "    (embeddings): StackedEmbeddings(\n",
      "      (list_embedding_0): WordEmbeddings('glove')\n",
      "    )\n",
      "    (word_reprojection_map): Linear(in_features=100, out_features=256, bias=True)\n",
      "    (rnn): GRU(256, 512, batch_first=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=512, out_features=115, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (beta): 1.0\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2020-11-06 11:58:01,250 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 11:58:01,251 Corpus: \"Corpus: 16000 train + 2000 dev + 2000 test sentences\"\n",
      "2020-11-06 11:58:01,253 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 11:58:01,254 Parameters:\n",
      "2020-11-06 11:58:01,256  - learning_rate: \"0.1\"\n",
      "2020-11-06 11:58:01,257  - mini_batch_size: \"32\"\n",
      "2020-11-06 11:58:01,258  - patience: \"8\"\n",
      "2020-11-06 11:58:01,261  - anneal_factor: \"0.5\"\n",
      "2020-11-06 11:58:01,263  - max_epochs: \"20\"\n",
      "2020-11-06 11:58:01,266  - shuffle: \"True\"\n",
      "2020-11-06 11:58:01,269  - train_with_dev: \"False\"\n",
      "2020-11-06 11:58:01,270  - batch_growth_annealing: \"False\"\n",
      "2020-11-06 11:58:01,272 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 11:58:01,273 Model training base path: \"model-saves2\"\n",
      "2020-11-06 11:58:01,275 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 11:58:01,278 Device: cpu\n",
      "2020-11-06 11:58:01,280 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 11:58:01,283 Embeddings storage mode: cpu\n",
      "2020-11-06 11:58:01,301 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 11:58:19,252 epoch 1 - iter 50/500 - loss 2.57097458 - samples/sec: 92.81 - lr: 0.100000\n",
      "2020-11-06 11:58:36,238 epoch 1 - iter 100/500 - loss 2.43749296 - samples/sec: 95.33 - lr: 0.100000\n",
      "2020-11-06 11:58:52,749 epoch 1 - iter 150/500 - loss 2.34139002 - samples/sec: 97.88 - lr: 0.100000\n",
      "2020-11-06 11:59:09,336 epoch 1 - iter 200/500 - loss 2.28386408 - samples/sec: 97.57 - lr: 0.100000\n",
      "2020-11-06 11:59:27,569 epoch 1 - iter 250/500 - loss 2.24226377 - samples/sec: 93.74 - lr: 0.100000\n",
      "2020-11-06 11:59:43,818 epoch 1 - iter 300/500 - loss 2.22266340 - samples/sec: 99.39 - lr: 0.100000\n",
      "2020-11-06 11:59:59,948 epoch 1 - iter 350/500 - loss 2.19829445 - samples/sec: 100.16 - lr: 0.100000\n",
      "2020-11-06 12:00:17,112 epoch 1 - iter 400/500 - loss 2.18044136 - samples/sec: 98.31 - lr: 0.100000\n",
      "2020-11-06 12:00:33,786 epoch 1 - iter 450/500 - loss 2.16606430 - samples/sec: 96.71 - lr: 0.100000\n",
      "2020-11-06 12:00:49,731 epoch 1 - iter 500/500 - loss 2.15837393 - samples/sec: 101.33 - lr: 0.100000\n",
      "2020-11-06 12:00:49,986 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:00:49,987 EPOCH 1 done: loss 2.1584 - lr 0.1000000\n",
      "2020-11-06 12:00:56,272 DEV : loss 1.945324420928955 - score 0.3843\n",
      "2020-11-06 12:00:58,084 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:01:01,010 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:01:18,497 epoch 2 - iter 50/500 - loss 2.01908566 - samples/sec: 94.71 - lr: 0.100000\n",
      "2020-11-06 12:01:35,506 epoch 2 - iter 100/500 - loss 2.05027696 - samples/sec: 95.04 - lr: 0.100000\n",
      "2020-11-06 12:01:52,253 epoch 2 - iter 150/500 - loss 2.03640675 - samples/sec: 96.55 - lr: 0.100000\n",
      "2020-11-06 12:02:09,582 epoch 2 - iter 200/500 - loss 2.03717676 - samples/sec: 97.88 - lr: 0.100000\n",
      "2020-11-06 12:02:25,853 epoch 2 - iter 250/500 - loss 2.03653683 - samples/sec: 99.11 - lr: 0.100000\n",
      "2020-11-06 12:02:42,440 epoch 2 - iter 300/500 - loss 2.02683621 - samples/sec: 97.48 - lr: 0.100000\n",
      "2020-11-06 12:02:59,555 epoch 2 - iter 350/500 - loss 2.02651623 - samples/sec: 98.49 - lr: 0.100000\n",
      "2020-11-06 12:03:16,154 epoch 2 - iter 400/500 - loss 2.02049549 - samples/sec: 97.36 - lr: 0.100000\n",
      "2020-11-06 12:03:32,852 epoch 2 - iter 450/500 - loss 2.02030875 - samples/sec: 96.71 - lr: 0.100000\n",
      "2020-11-06 12:03:50,137 epoch 2 - iter 500/500 - loss 2.00700917 - samples/sec: 93.34 - lr: 0.100000\n",
      "2020-11-06 12:03:50,398 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:03:50,400 EPOCH 2 done: loss 2.0070 - lr 0.1000000\n",
      "2020-11-06 12:04:01,502 DEV : loss 1.9326533079147339 - score 0.3552\n",
      "2020-11-06 12:04:02,555 BAD EPOCHS (no improvement): 1\n",
      "2020-11-06 12:04:02,557 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:04:21,059 epoch 3 - iter 50/500 - loss 1.86100336 - samples/sec: 89.10 - lr: 0.100000\n",
      "2020-11-06 12:04:40,328 epoch 3 - iter 100/500 - loss 1.87590632 - samples/sec: 89.60 - lr: 0.100000\n",
      "2020-11-06 12:04:56,946 epoch 3 - iter 150/500 - loss 1.87390047 - samples/sec: 97.18 - lr: 0.100000\n",
      "2020-11-06 12:05:13,836 epoch 3 - iter 200/500 - loss 1.87483949 - samples/sec: 95.68 - lr: 0.100000\n",
      "2020-11-06 12:05:31,034 epoch 3 - iter 250/500 - loss 1.86068327 - samples/sec: 93.75 - lr: 0.100000\n",
      "2020-11-06 12:05:48,271 epoch 3 - iter 300/500 - loss 1.86195658 - samples/sec: 98.33 - lr: 0.100000\n",
      "2020-11-06 12:06:04,449 epoch 3 - iter 350/500 - loss 1.85559579 - samples/sec: 99.82 - lr: 0.100000\n",
      "2020-11-06 12:06:21,656 epoch 3 - iter 400/500 - loss 1.85028467 - samples/sec: 93.81 - lr: 0.100000\n",
      "2020-11-06 12:06:39,823 epoch 3 - iter 450/500 - loss 1.84182256 - samples/sec: 93.18 - lr: 0.100000\n",
      "2020-11-06 12:06:55,792 epoch 3 - iter 500/500 - loss 1.83026681 - samples/sec: 101.09 - lr: 0.100000\n",
      "2020-11-06 12:06:56,059 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:06:56,060 EPOCH 3 done: loss 1.8303 - lr 0.1000000\n",
      "2020-11-06 12:07:02,502 DEV : loss 1.559191107749939 - score 0.521\n",
      "2020-11-06 12:07:03,548 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:07:06,653 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:07:25,755 epoch 4 - iter 50/500 - loss 1.72776531 - samples/sec: 91.97 - lr: 0.100000\n",
      "2020-11-06 12:07:42,680 epoch 4 - iter 100/500 - loss 1.69738607 - samples/sec: 95.29 - lr: 0.100000\n",
      "2020-11-06 12:07:59,459 epoch 4 - iter 150/500 - loss 1.71603585 - samples/sec: 96.35 - lr: 0.100000\n",
      "2020-11-06 12:08:17,244 epoch 4 - iter 200/500 - loss 1.71579447 - samples/sec: 94.81 - lr: 0.100000\n",
      "2020-11-06 12:08:32,626 epoch 4 - iter 250/500 - loss 1.70793024 - samples/sec: 105.15 - lr: 0.100000\n",
      "2020-11-06 12:08:49,083 epoch 4 - iter 300/500 - loss 1.69489065 - samples/sec: 98.11 - lr: 0.100000\n",
      "2020-11-06 12:09:05,634 epoch 4 - iter 350/500 - loss 1.67481978 - samples/sec: 97.57 - lr: 0.100000\n",
      "2020-11-06 12:09:22,961 epoch 4 - iter 400/500 - loss 1.65918218 - samples/sec: 97.63 - lr: 0.100000\n",
      "2020-11-06 12:09:38,958 epoch 4 - iter 450/500 - loss 1.66065150 - samples/sec: 100.84 - lr: 0.100000\n",
      "2020-11-06 12:09:55,397 epoch 4 - iter 500/500 - loss 1.65379627 - samples/sec: 98.23 - lr: 0.100000\n",
      "2020-11-06 12:09:55,678 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:09:55,679 EPOCH 4 done: loss 1.6538 - lr 0.1000000\n",
      "2020-11-06 12:10:03,058 DEV : loss 1.3066920042037964 - score 0.6278\n",
      "2020-11-06 12:10:04,119 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:10:07,029 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:10:25,698 epoch 5 - iter 50/500 - loss 1.52537971 - samples/sec: 88.61 - lr: 0.100000\n",
      "2020-11-06 12:10:42,102 epoch 5 - iter 100/500 - loss 1.52983005 - samples/sec: 98.54 - lr: 0.100000\n",
      "2020-11-06 12:10:59,921 epoch 5 - iter 150/500 - loss 1.53042288 - samples/sec: 95.71 - lr: 0.100000\n",
      "2020-11-06 12:11:15,695 epoch 5 - iter 200/500 - loss 1.53236114 - samples/sec: 102.40 - lr: 0.100000\n",
      "2020-11-06 12:11:32,118 epoch 5 - iter 250/500 - loss 1.54185416 - samples/sec: 98.47 - lr: 0.100000\n",
      "2020-11-06 12:11:51,782 epoch 5 - iter 300/500 - loss 1.53931142 - samples/sec: 85.90 - lr: 0.100000\n",
      "2020-11-06 12:12:09,005 epoch 5 - iter 350/500 - loss 1.53757876 - samples/sec: 93.74 - lr: 0.100000\n",
      "2020-11-06 12:12:25,570 epoch 5 - iter 400/500 - loss 1.53448721 - samples/sec: 97.50 - lr: 0.100000\n",
      "2020-11-06 12:12:42,409 epoch 5 - iter 450/500 - loss 1.52395412 - samples/sec: 95.81 - lr: 0.100000\n",
      "2020-11-06 12:13:00,080 epoch 5 - iter 500/500 - loss 1.52110793 - samples/sec: 95.79 - lr: 0.100000\n",
      "2020-11-06 12:13:00,347 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:13:00,349 EPOCH 5 done: loss 1.5211 - lr 0.1000000\n",
      "2020-11-06 12:13:06,606 DEV : loss 1.1271493434906006 - score 0.6784\n",
      "2020-11-06 12:13:07,662 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:13:10,628 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:13:28,933 epoch 6 - iter 50/500 - loss 1.47741023 - samples/sec: 90.59 - lr: 0.100000\n",
      "2020-11-06 12:13:45,728 epoch 6 - iter 100/500 - loss 1.41578101 - samples/sec: 102.09 - lr: 0.100000\n",
      "2020-11-06 12:14:02,023 epoch 6 - iter 150/500 - loss 1.43158411 - samples/sec: 99.07 - lr: 0.100000\n",
      "2020-11-06 12:14:18,533 epoch 6 - iter 200/500 - loss 1.42044646 - samples/sec: 97.80 - lr: 0.100000\n",
      "2020-11-06 12:14:36,173 epoch 6 - iter 250/500 - loss 1.41432852 - samples/sec: 95.90 - lr: 0.100000\n",
      "2020-11-06 12:14:52,494 epoch 6 - iter 300/500 - loss 1.41455203 - samples/sec: 99.01 - lr: 0.100000\n",
      "2020-11-06 12:15:08,714 epoch 6 - iter 350/500 - loss 1.40923518 - samples/sec: 99.49 - lr: 0.100000\n",
      "2020-11-06 12:15:26,668 epoch 6 - iter 400/500 - loss 1.41275906 - samples/sec: 94.06 - lr: 0.100000\n",
      "2020-11-06 12:15:42,055 epoch 6 - iter 450/500 - loss 1.41880573 - samples/sec: 105.10 - lr: 0.100000\n",
      "2020-11-06 12:15:58,177 epoch 6 - iter 500/500 - loss 1.41913165 - samples/sec: 100.07 - lr: 0.100000\n",
      "2020-11-06 12:15:58,451 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:15:58,453 EPOCH 6 done: loss 1.4191 - lr 0.1000000\n",
      "2020-11-06 12:16:05,569 DEV : loss 1.0238040685653687 - score 0.6954\n",
      "2020-11-06 12:16:06,626 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:16:09,583 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:16:26,798 epoch 7 - iter 50/500 - loss 1.39911625 - samples/sec: 96.54 - lr: 0.100000\n",
      "2020-11-06 12:16:43,669 epoch 7 - iter 100/500 - loss 1.39165818 - samples/sec: 95.79 - lr: 0.100000\n",
      "2020-11-06 12:17:01,094 epoch 7 - iter 150/500 - loss 1.36250679 - samples/sec: 92.86 - lr: 0.100000\n",
      "2020-11-06 12:17:19,035 epoch 7 - iter 200/500 - loss 1.36747258 - samples/sec: 95.07 - lr: 0.100000\n",
      "2020-11-06 12:17:35,554 epoch 7 - iter 250/500 - loss 1.36478360 - samples/sec: 97.66 - lr: 0.100000\n",
      "2020-11-06 12:17:51,867 epoch 7 - iter 300/500 - loss 1.35678568 - samples/sec: 99.08 - lr: 0.100000\n",
      "2020-11-06 12:18:10,080 epoch 7 - iter 350/500 - loss 1.34636852 - samples/sec: 92.81 - lr: 0.100000\n",
      "2020-11-06 12:18:27,516 epoch 7 - iter 400/500 - loss 1.34473638 - samples/sec: 92.53 - lr: 0.100000\n",
      "2020-11-06 12:18:44,228 epoch 7 - iter 450/500 - loss 1.33634352 - samples/sec: 96.74 - lr: 0.100000\n",
      "2020-11-06 12:19:00,978 epoch 7 - iter 500/500 - loss 1.33548709 - samples/sec: 96.34 - lr: 0.100000\n",
      "2020-11-06 12:19:01,246 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:19:01,247 EPOCH 7 done: loss 1.3355 - lr 0.1000000\n",
      "2020-11-06 12:19:08,635 DEV : loss 0.9342482089996338 - score 0.7255\n",
      "2020-11-06 12:19:09,678 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:19:12,563 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:19:29,974 epoch 8 - iter 50/500 - loss 1.23599043 - samples/sec: 95.39 - lr: 0.100000\n",
      "2020-11-06 12:19:46,660 epoch 8 - iter 100/500 - loss 1.27070746 - samples/sec: 96.96 - lr: 0.100000\n",
      "2020-11-06 12:20:03,504 epoch 8 - iter 150/500 - loss 1.26187886 - samples/sec: 101.52 - lr: 0.100000\n",
      "2020-11-06 12:20:20,950 epoch 8 - iter 200/500 - loss 1.26422703 - samples/sec: 92.53 - lr: 0.100000\n",
      "2020-11-06 12:20:37,702 epoch 8 - iter 250/500 - loss 1.26759021 - samples/sec: 96.45 - lr: 0.100000\n",
      "2020-11-06 12:20:55,390 epoch 8 - iter 300/500 - loss 1.25641730 - samples/sec: 95.44 - lr: 0.100000\n",
      "2020-11-06 12:21:12,647 epoch 8 - iter 350/500 - loss 1.25692656 - samples/sec: 93.54 - lr: 0.100000\n",
      "2020-11-06 12:21:29,079 epoch 8 - iter 400/500 - loss 1.25773489 - samples/sec: 98.37 - lr: 0.100000\n",
      "2020-11-06 12:21:47,149 epoch 8 - iter 450/500 - loss 1.26463590 - samples/sec: 93.37 - lr: 0.100000\n",
      "2020-11-06 12:22:03,693 epoch 8 - iter 500/500 - loss 1.26521650 - samples/sec: 97.61 - lr: 0.100000\n",
      "2020-11-06 12:22:03,949 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:22:03,951 EPOCH 8 done: loss 1.2652 - lr 0.1000000\n",
      "2020-11-06 12:22:10,570 DEV : loss 0.8552535176277161 - score 0.7495\n",
      "2020-11-06 12:22:11,623 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:22:14,675 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:22:34,025 epoch 9 - iter 50/500 - loss 1.18687392 - samples/sec: 90.13 - lr: 0.100000\n",
      "2020-11-06 12:22:51,103 epoch 9 - iter 100/500 - loss 1.20099691 - samples/sec: 94.56 - lr: 0.100000\n",
      "2020-11-06 12:23:07,847 epoch 9 - iter 150/500 - loss 1.20801715 - samples/sec: 96.42 - lr: 0.100000\n",
      "2020-11-06 12:23:25,491 epoch 9 - iter 200/500 - loss 1.20386092 - samples/sec: 95.81 - lr: 0.100000\n",
      "2020-11-06 12:23:42,053 epoch 9 - iter 250/500 - loss 1.21049149 - samples/sec: 97.52 - lr: 0.100000\n",
      "2020-11-06 12:23:58,414 epoch 9 - iter 300/500 - loss 1.21023610 - samples/sec: 98.67 - lr: 0.100000\n",
      "2020-11-06 12:24:15,822 epoch 9 - iter 350/500 - loss 1.21240570 - samples/sec: 92.78 - lr: 0.100000\n",
      "2020-11-06 12:24:33,601 epoch 9 - iter 400/500 - loss 1.20683331 - samples/sec: 95.04 - lr: 0.100000\n",
      "2020-11-06 12:24:50,283 epoch 9 - iter 450/500 - loss 1.19888511 - samples/sec: 96.93 - lr: 0.100000\n",
      "2020-11-06 12:25:06,955 epoch 9 - iter 500/500 - loss 1.19594561 - samples/sec: 96.85 - lr: 0.100000\n",
      "2020-11-06 12:25:07,224 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:25:07,225 EPOCH 9 done: loss 1.1959 - lr 0.1000000\n",
      "2020-11-06 12:25:14,624 DEV : loss 0.8065276145935059 - score 0.754\n",
      "2020-11-06 12:25:15,686 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:25:18,596 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:25:35,305 epoch 10 - iter 50/500 - loss 1.13747095 - samples/sec: 99.57 - lr: 0.100000\n",
      "2020-11-06 12:25:52,011 epoch 10 - iter 100/500 - loss 1.15298059 - samples/sec: 96.98 - lr: 0.100000\n",
      "2020-11-06 12:26:09,227 epoch 10 - iter 150/500 - loss 1.15891265 - samples/sec: 99.15 - lr: 0.100000\n",
      "2020-11-06 12:26:25,985 epoch 10 - iter 200/500 - loss 1.15858173 - samples/sec: 96.29 - lr: 0.100000\n",
      "2020-11-06 12:26:42,482 epoch 10 - iter 250/500 - loss 1.16919248 - samples/sec: 98.07 - lr: 0.100000\n",
      "2020-11-06 12:27:00,820 epoch 10 - iter 300/500 - loss 1.16668344 - samples/sec: 87.98 - lr: 0.100000\n",
      "2020-11-06 12:27:17,835 epoch 10 - iter 350/500 - loss 1.16709136 - samples/sec: 99.81 - lr: 0.100000\n",
      "2020-11-06 12:27:33,991 epoch 10 - iter 400/500 - loss 1.16363281 - samples/sec: 99.98 - lr: 0.100000\n",
      "2020-11-06 12:27:50,764 epoch 10 - iter 450/500 - loss 1.15959731 - samples/sec: 96.23 - lr: 0.100000\n",
      "2020-11-06 12:28:08,170 epoch 10 - iter 500/500 - loss 1.15805719 - samples/sec: 96.96 - lr: 0.100000\n",
      "2020-11-06 12:28:08,437 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:28:08,438 EPOCH 10 done: loss 1.1581 - lr 0.1000000\n",
      "2020-11-06 12:28:14,878 DEV : loss 0.739918053150177 - score 0.7776\n",
      "2020-11-06 12:28:15,947 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:28:19,397 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:28:36,972 epoch 11 - iter 50/500 - loss 1.13292658 - samples/sec: 94.27 - lr: 0.100000\n",
      "2020-11-06 12:28:55,200 epoch 11 - iter 100/500 - loss 1.13418781 - samples/sec: 93.73 - lr: 0.100000\n",
      "2020-11-06 12:29:11,242 epoch 11 - iter 150/500 - loss 1.12210873 - samples/sec: 100.60 - lr: 0.100000\n",
      "2020-11-06 12:29:27,939 epoch 11 - iter 200/500 - loss 1.12193662 - samples/sec: 96.72 - lr: 0.100000\n",
      "2020-11-06 12:29:45,501 epoch 11 - iter 250/500 - loss 1.11605024 - samples/sec: 96.16 - lr: 0.100000\n",
      "2020-11-06 12:30:02,561 epoch 11 - iter 300/500 - loss 1.11586026 - samples/sec: 94.64 - lr: 0.100000\n",
      "2020-11-06 12:30:18,806 epoch 11 - iter 350/500 - loss 1.11136722 - samples/sec: 99.39 - lr: 0.100000\n",
      "2020-11-06 12:30:36,331 epoch 11 - iter 400/500 - loss 1.11073367 - samples/sec: 92.14 - lr: 0.100000\n",
      "2020-11-06 12:30:53,514 epoch 11 - iter 450/500 - loss 1.11013553 - samples/sec: 98.51 - lr: 0.100000\n",
      "2020-11-06 12:31:09,724 epoch 11 - iter 500/500 - loss 1.10747394 - samples/sec: 99.48 - lr: 0.100000\n",
      "2020-11-06 12:31:09,982 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:31:09,983 EPOCH 11 done: loss 1.1075 - lr 0.1000000\n",
      "2020-11-06 12:31:16,353 DEV : loss 0.703685462474823 - score 0.7876\n",
      "2020-11-06 12:31:17,416 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:31:20,422 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:31:43,960 epoch 12 - iter 50/500 - loss 1.04586506 - samples/sec: 91.51 - lr: 0.100000\n",
      "2020-11-06 12:32:00,673 epoch 12 - iter 100/500 - loss 1.06480285 - samples/sec: 96.59 - lr: 0.100000\n",
      "2020-11-06 12:32:18,109 epoch 12 - iter 150/500 - loss 1.07491330 - samples/sec: 92.62 - lr: 0.100000\n",
      "2020-11-06 12:32:35,176 epoch 12 - iter 200/500 - loss 1.08332959 - samples/sec: 99.36 - lr: 0.100000\n",
      "2020-11-06 12:32:51,314 epoch 12 - iter 250/500 - loss 1.08702992 - samples/sec: 100.20 - lr: 0.100000\n",
      "2020-11-06 12:33:08,412 epoch 12 - iter 300/500 - loss 1.09633537 - samples/sec: 94.39 - lr: 0.100000\n",
      "2020-11-06 12:33:27,031 epoch 12 - iter 350/500 - loss 1.09345416 - samples/sec: 90.48 - lr: 0.100000\n",
      "2020-11-06 12:33:43,454 epoch 12 - iter 400/500 - loss 1.09015155 - samples/sec: 98.48 - lr: 0.100000\n",
      "2020-11-06 12:33:59,668 epoch 12 - iter 450/500 - loss 1.08411142 - samples/sec: 99.67 - lr: 0.100000\n",
      "2020-11-06 12:34:16,540 epoch 12 - iter 500/500 - loss 1.07962745 - samples/sec: 95.71 - lr: 0.100000\n",
      "2020-11-06 12:34:16,922 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:34:16,924 EPOCH 12 done: loss 1.0796 - lr 0.1000000\n",
      "2020-11-06 12:34:24,597 DEV : loss 0.6572102904319763 - score 0.8001\n",
      "2020-11-06 12:34:25,656 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:34:28,667 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:34:45,287 epoch 13 - iter 50/500 - loss 1.03868761 - samples/sec: 100.13 - lr: 0.100000\n",
      "2020-11-06 12:35:05,514 epoch 13 - iter 100/500 - loss 1.05481515 - samples/sec: 80.10 - lr: 0.100000\n",
      "2020-11-06 12:35:24,218 epoch 13 - iter 150/500 - loss 1.05095002 - samples/sec: 91.81 - lr: 0.100000\n",
      "2020-11-06 12:35:41,412 epoch 13 - iter 200/500 - loss 1.08119088 - samples/sec: 93.98 - lr: 0.100000\n",
      "2020-11-06 12:35:59,343 epoch 13 - iter 250/500 - loss 1.07304791 - samples/sec: 90.01 - lr: 0.100000\n",
      "2020-11-06 12:36:17,806 epoch 13 - iter 300/500 - loss 1.06351955 - samples/sec: 91.36 - lr: 0.100000\n",
      "2020-11-06 12:36:34,542 epoch 13 - iter 350/500 - loss 1.06850824 - samples/sec: 96.49 - lr: 0.100000\n",
      "2020-11-06 12:36:51,368 epoch 13 - iter 400/500 - loss 1.07094082 - samples/sec: 95.99 - lr: 0.100000\n",
      "2020-11-06 12:37:08,918 epoch 13 - iter 450/500 - loss 1.07174212 - samples/sec: 96.58 - lr: 0.100000\n",
      "2020-11-06 12:37:25,744 epoch 13 - iter 500/500 - loss 1.07277668 - samples/sec: 95.94 - lr: 0.100000\n",
      "2020-11-06 12:37:26,024 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:37:26,025 EPOCH 13 done: loss 1.0728 - lr 0.1000000\n",
      "2020-11-06 12:37:32,292 DEV : loss 0.625537097454071 - score 0.7981\n",
      "2020-11-06 12:37:33,346 BAD EPOCHS (no improvement): 1\n",
      "2020-11-06 12:37:33,347 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:37:52,580 epoch 14 - iter 50/500 - loss 1.03166781 - samples/sec: 90.83 - lr: 0.100000\n",
      "2020-11-06 12:38:09,281 epoch 14 - iter 100/500 - loss 1.04469195 - samples/sec: 96.80 - lr: 0.100000\n",
      "2020-11-06 12:38:26,034 epoch 14 - iter 150/500 - loss 1.03798657 - samples/sec: 96.35 - lr: 0.100000\n",
      "2020-11-06 12:38:43,753 epoch 14 - iter 200/500 - loss 1.04211532 - samples/sec: 91.11 - lr: 0.100000\n",
      "2020-11-06 12:39:01,773 epoch 14 - iter 250/500 - loss 1.03457545 - samples/sec: 93.84 - lr: 0.100000\n",
      "2020-11-06 12:39:18,182 epoch 14 - iter 300/500 - loss 1.03826181 - samples/sec: 98.43 - lr: 0.100000\n",
      "2020-11-06 12:39:35,683 epoch 14 - iter 350/500 - loss 1.03946111 - samples/sec: 92.30 - lr: 0.100000\n",
      "2020-11-06 12:39:53,814 epoch 14 - iter 400/500 - loss 1.04488230 - samples/sec: 93.42 - lr: 0.100000\n",
      "2020-11-06 12:40:09,970 epoch 14 - iter 450/500 - loss 1.03991093 - samples/sec: 99.94 - lr: 0.100000\n",
      "2020-11-06 12:40:26,234 epoch 14 - iter 500/500 - loss 1.03866872 - samples/sec: 99.32 - lr: 0.100000\n",
      "2020-11-06 12:40:26,525 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:40:26,526 EPOCH 14 done: loss 1.0387 - lr 0.1000000\n",
      "2020-11-06 12:40:34,040 DEV : loss 0.6310291886329651 - score 0.7976\n",
      "2020-11-06 12:40:35,099 BAD EPOCHS (no improvement): 2\n",
      "2020-11-06 12:40:35,100 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:40:52,985 epoch 15 - iter 50/500 - loss 0.97656172 - samples/sec: 92.68 - lr: 0.100000\n",
      "2020-11-06 12:41:10,508 epoch 15 - iter 100/500 - loss 0.97708119 - samples/sec: 92.33 - lr: 0.100000\n",
      "2020-11-06 12:41:29,713 epoch 15 - iter 150/500 - loss 0.97738661 - samples/sec: 88.52 - lr: 0.100000\n",
      "2020-11-06 12:41:47,193 epoch 15 - iter 200/500 - loss 0.98203698 - samples/sec: 92.37 - lr: 0.100000\n",
      "2020-11-06 12:42:04,114 epoch 15 - iter 250/500 - loss 0.98710061 - samples/sec: 95.51 - lr: 0.100000\n",
      "2020-11-06 12:42:22,272 epoch 15 - iter 300/500 - loss 0.98657579 - samples/sec: 88.96 - lr: 0.100000\n",
      "2020-11-06 12:42:41,094 epoch 15 - iter 350/500 - loss 0.99483074 - samples/sec: 89.78 - lr: 0.100000\n",
      "2020-11-06 12:42:56,953 epoch 15 - iter 400/500 - loss 0.99406406 - samples/sec: 101.73 - lr: 0.100000\n",
      "2020-11-06 12:43:14,065 epoch 15 - iter 450/500 - loss 0.99347723 - samples/sec: 94.46 - lr: 0.100000\n",
      "2020-11-06 12:43:32,185 epoch 15 - iter 500/500 - loss 0.99716119 - samples/sec: 93.35 - lr: 0.100000\n",
      "2020-11-06 12:43:32,459 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:43:32,461 EPOCH 15 done: loss 0.9972 - lr 0.1000000\n",
      "2020-11-06 12:43:38,851 DEV : loss 0.614797055721283 - score 0.8086\n",
      "2020-11-06 12:43:39,913 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:43:42,922 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:44:01,278 epoch 16 - iter 50/500 - loss 0.97871427 - samples/sec: 90.35 - lr: 0.100000\n",
      "2020-11-06 12:44:19,879 epoch 16 - iter 100/500 - loss 1.00215350 - samples/sec: 91.81 - lr: 0.100000\n",
      "2020-11-06 12:44:37,342 epoch 16 - iter 150/500 - loss 0.97487285 - samples/sec: 92.56 - lr: 0.100000\n",
      "2020-11-06 12:44:54,181 epoch 16 - iter 200/500 - loss 0.97762212 - samples/sec: 95.88 - lr: 0.100000\n",
      "2020-11-06 12:45:11,176 epoch 16 - iter 250/500 - loss 0.97038741 - samples/sec: 95.13 - lr: 0.100000\n",
      "2020-11-06 12:45:29,055 epoch 16 - iter 300/500 - loss 0.96600464 - samples/sec: 94.73 - lr: 0.100000\n",
      "2020-11-06 12:45:45,424 epoch 16 - iter 350/500 - loss 0.96826409 - samples/sec: 98.72 - lr: 0.100000\n",
      "2020-11-06 12:46:02,848 epoch 16 - iter 400/500 - loss 0.97427474 - samples/sec: 92.64 - lr: 0.100000\n",
      "2020-11-06 12:46:20,382 epoch 16 - iter 450/500 - loss 0.97469542 - samples/sec: 96.93 - lr: 0.100000\n",
      "2020-11-06 12:46:37,335 epoch 16 - iter 500/500 - loss 0.98098612 - samples/sec: 95.17 - lr: 0.100000\n",
      "2020-11-06 12:46:37,624 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:46:37,626 EPOCH 16 done: loss 0.9810 - lr 0.1000000\n",
      "2020-11-06 12:46:43,979 DEV : loss 0.5761505365371704 - score 0.8136\n",
      "2020-11-06 12:46:45,039 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:46:48,045 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:47:07,474 epoch 17 - iter 50/500 - loss 0.89327663 - samples/sec: 90.14 - lr: 0.100000\n",
      "2020-11-06 12:47:24,113 epoch 17 - iter 100/500 - loss 0.91290242 - samples/sec: 97.12 - lr: 0.100000\n",
      "2020-11-06 12:47:41,905 epoch 17 - iter 150/500 - loss 0.91691678 - samples/sec: 90.76 - lr: 0.100000\n",
      "2020-11-06 12:48:00,044 epoch 17 - iter 200/500 - loss 0.92236406 - samples/sec: 93.57 - lr: 0.100000\n",
      "2020-11-06 12:48:17,211 epoch 17 - iter 250/500 - loss 0.93692474 - samples/sec: 94.14 - lr: 0.100000\n",
      "2020-11-06 12:48:33,235 epoch 17 - iter 300/500 - loss 0.93392476 - samples/sec: 100.74 - lr: 0.100000\n",
      "2020-11-06 12:48:51,269 epoch 17 - iter 350/500 - loss 0.94474662 - samples/sec: 93.85 - lr: 0.100000\n",
      "2020-11-06 12:49:08,941 epoch 17 - iter 400/500 - loss 0.95056534 - samples/sec: 91.27 - lr: 0.100000\n",
      "2020-11-06 12:49:25,332 epoch 17 - iter 450/500 - loss 0.94899617 - samples/sec: 98.66 - lr: 0.100000\n",
      "2020-11-06 12:49:42,181 epoch 17 - iter 500/500 - loss 0.95100898 - samples/sec: 95.82 - lr: 0.100000\n",
      "2020-11-06 12:49:42,471 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:49:42,472 EPOCH 17 done: loss 0.9510 - lr 0.1000000\n",
      "2020-11-06 12:49:49,985 DEV : loss 0.549270749092102 - score 0.8317\n",
      "2020-11-06 12:49:51,045 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:49:54,001 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:50:11,531 epoch 18 - iter 50/500 - loss 0.92178810 - samples/sec: 94.58 - lr: 0.100000\n",
      "2020-11-06 12:50:28,682 epoch 18 - iter 100/500 - loss 0.93869121 - samples/sec: 94.25 - lr: 0.100000\n",
      "2020-11-06 12:50:46,258 epoch 18 - iter 150/500 - loss 0.93369113 - samples/sec: 97.09 - lr: 0.100000\n",
      "2020-11-06 12:51:02,471 epoch 18 - iter 200/500 - loss 0.91655966 - samples/sec: 99.56 - lr: 0.100000\n",
      "2020-11-06 12:51:20,312 epoch 18 - iter 250/500 - loss 0.91908533 - samples/sec: 90.62 - lr: 0.100000\n",
      "2020-11-06 12:51:38,291 epoch 18 - iter 300/500 - loss 0.92287556 - samples/sec: 94.20 - lr: 0.100000\n",
      "2020-11-06 12:51:55,440 epoch 18 - iter 350/500 - loss 0.93508736 - samples/sec: 94.29 - lr: 0.100000\n",
      "2020-11-06 12:52:12,569 epoch 18 - iter 400/500 - loss 0.93808488 - samples/sec: 94.22 - lr: 0.100000\n",
      "2020-11-06 12:52:29,821 epoch 18 - iter 450/500 - loss 0.93657175 - samples/sec: 93.59 - lr: 0.100000\n",
      "2020-11-06 12:52:47,595 epoch 18 - iter 500/500 - loss 0.93519677 - samples/sec: 95.19 - lr: 0.100000\n",
      "2020-11-06 12:52:47,901 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:52:47,902 EPOCH 18 done: loss 0.9352 - lr 0.1000000\n",
      "2020-11-06 12:52:54,375 DEV : loss 0.5605600476264954 - score 0.8246\n",
      "2020-11-06 12:52:55,461 BAD EPOCHS (no improvement): 1\n",
      "2020-11-06 12:52:55,463 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:53:13,899 epoch 19 - iter 50/500 - loss 0.93179008 - samples/sec: 89.66 - lr: 0.100000\n",
      "2020-11-06 12:53:32,935 epoch 19 - iter 100/500 - loss 0.93660043 - samples/sec: 89.77 - lr: 0.100000\n",
      "2020-11-06 12:53:49,810 epoch 19 - iter 150/500 - loss 0.95070270 - samples/sec: 95.71 - lr: 0.100000\n",
      "2020-11-06 12:54:06,635 epoch 19 - iter 200/500 - loss 0.95685212 - samples/sec: 95.91 - lr: 0.100000\n",
      "2020-11-06 12:54:24,456 epoch 19 - iter 250/500 - loss 0.94914644 - samples/sec: 95.31 - lr: 0.100000\n",
      "2020-11-06 12:54:41,776 epoch 19 - iter 300/500 - loss 0.94027742 - samples/sec: 93.17 - lr: 0.100000\n",
      "2020-11-06 12:54:58,702 epoch 19 - iter 350/500 - loss 0.93514584 - samples/sec: 95.45 - lr: 0.100000\n",
      "2020-11-06 12:55:15,327 epoch 19 - iter 400/500 - loss 0.92897928 - samples/sec: 97.14 - lr: 0.100000\n",
      "2020-11-06 12:55:34,217 epoch 19 - iter 450/500 - loss 0.92818892 - samples/sec: 90.49 - lr: 0.100000\n",
      "2020-11-06 12:55:51,838 epoch 19 - iter 500/500 - loss 0.92524604 - samples/sec: 91.56 - lr: 0.100000\n",
      "2020-11-06 12:55:52,135 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:55:52,137 EPOCH 19 done: loss 0.9252 - lr 0.1000000\n",
      "2020-11-06 12:55:59,283 DEV : loss 0.5371593832969666 - score 0.8372\n",
      "2020-11-06 12:56:00,331 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:56:03,286 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:56:21,824 epoch 20 - iter 50/500 - loss 0.92781196 - samples/sec: 89.44 - lr: 0.100000\n",
      "2020-11-06 12:56:38,905 epoch 20 - iter 100/500 - loss 0.89189171 - samples/sec: 94.76 - lr: 0.100000\n",
      "2020-11-06 12:56:56,858 epoch 20 - iter 150/500 - loss 0.88976469 - samples/sec: 90.25 - lr: 0.100000\n",
      "2020-11-06 12:57:14,590 epoch 20 - iter 200/500 - loss 0.89728430 - samples/sec: 95.95 - lr: 0.100000\n",
      "2020-11-06 12:57:31,250 epoch 20 - iter 250/500 - loss 0.90035624 - samples/sec: 97.12 - lr: 0.100000\n",
      "2020-11-06 12:57:48,846 epoch 20 - iter 300/500 - loss 0.90121571 - samples/sec: 91.75 - lr: 0.100000\n",
      "2020-11-06 12:58:07,206 epoch 20 - iter 350/500 - loss 0.90745994 - samples/sec: 92.26 - lr: 0.100000\n",
      "2020-11-06 12:58:24,579 epoch 20 - iter 400/500 - loss 0.90839270 - samples/sec: 92.89 - lr: 0.100000\n",
      "2020-11-06 12:58:40,983 epoch 20 - iter 450/500 - loss 0.90980429 - samples/sec: 98.61 - lr: 0.100000\n",
      "2020-11-06 12:58:58,593 epoch 20 - iter 500/500 - loss 0.91056679 - samples/sec: 91.61 - lr: 0.100000\n",
      "2020-11-06 12:58:58,872 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:58:58,874 EPOCH 20 done: loss 0.9106 - lr 0.1000000\n",
      "2020-11-06 12:59:06,487 DEV : loss 0.5069844126701355 - score 0.8387\n",
      "2020-11-06 12:59:07,569 BAD EPOCHS (no improvement): 0\n",
      "saving best model\n",
      "2020-11-06 12:59:13,406 ----------------------------------------------------------------------------------------------------\n",
      "2020-11-06 12:59:13,408 Testing using best model ...\n",
      "2020-11-06 12:59:13,409 loading file model-saves2/best-model.pt\n",
      "2020-11-06 12:59:21,393 \t0.8475\n",
      "2020-11-06 12:59:21,395 \n",
      "Results:\n",
      "- F-score (micro) 0.8475\n",
      "- F-score (macro) 0.0806\n",
      "- Accuracy 0.8475\n",
      "\n",
      "By class:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          4,i     0.8668    0.9295    0.8971       511\n",
      "         0,im     0.8235    0.6087    0.7000        23\n",
      "          3,i     0.7586    0.7639    0.7612       144\n",
      "          0,i     0.8982    0.8423    0.8694       241\n",
      "        4,ive     1.0000    0.0625    0.1176        16\n",
      "        5,ive     0.0000    0.0000    0.0000         1\n",
      "          1,i     0.8291    0.8291    0.8291       199\n",
      "          2,i     0.8780    0.9201    0.8986       626\n",
      "          5,i     0.6522    0.7258    0.6870        62\n",
      "        2,ive     0.0000    0.0000    0.0000         7\n",
      "         2,on     0.0000    0.0000    0.0000         0\n",
      "         4,im     0.8043    0.7255    0.7629        51\n",
      "       4,when     0.0000    0.0000    0.0000         1\n",
      "         2,im     0.8727    0.8571    0.8649        56\n",
      "    2,finding     0.0000    0.0000    0.0000         0\n",
      "         5,im     1.0000    0.3333    0.5000         3\n",
      "         1,im     0.7368    0.8235    0.7778        17\n",
      "       1,when     0.0000    0.0000    0.0000         0\n",
      "         0,id     0.0000    0.0000    0.0000         0\n",
      "         4,id     0.0000    0.0000    0.0000         1\n",
      "       1,fear     0.0000    0.0000    0.0000         0\n",
      "        2,ill     0.0000    0.0000    0.0000         2\n",
      "         2,id     0.0000    0.0000    0.0000         0\n",
      "         3,im     0.6667    0.5455    0.6000        11\n",
      "        1,ive     0.0000    0.0000    0.0000         6\n",
      "         3,is     0.0000    0.0000    0.0000         0\n",
      "        4,the     0.0000    0.0000    0.0000         0\n",
      "       0,when     0.0000    0.0000    0.0000         3\n",
      "        0,ive     0.0000    0.0000    0.0000         4\n",
      "       2,when     0.0000    0.0000    0.0000         0\n",
      "         0,my     0.0000    0.0000    0.0000         0\n",
      "        1,ill     0.0000    0.0000    0.0000         1\n",
      "       0,this     0.0000    0.0000    0.0000         0\n",
      "         2,my     0.0000    0.0000    0.0000         0\n",
      "         1,as     0.0000    0.0000    0.0000         0\n",
      "         4,my     0.0000    0.0000    0.0000         0\n",
      "    1,waiting     0.0000    0.0000    0.0000         0\n",
      "         3,id     0.0000    0.0000    0.0000         0\n",
      "          0,a     0.0000    0.0000    0.0000         0\n",
      "    2,imdoing     0.0000    0.0000    0.0000         0\n",
      "      1,while     0.0000    0.0000    0.0000         0\n",
      "          4,a     0.0000    0.0000    0.0000         0\n",
      "        3,ive     0.0000    0.0000    0.0000         4\n",
      "         0,we     0.0000    0.0000    0.0000         0\n",
      "         2,is     0.0000    0.0000    0.0000         1\n",
      "        4,ill     0.0000    0.0000    0.0000         0\n",
      "   0,sometime     0.0000    0.0000    0.0000         0\n",
      "        0,ill     0.0000    0.0000    0.0000         1\n",
      "      2,there     0.0000    0.0000    0.0000         0\n",
      "         0,in     0.0000    0.0000    0.0000         0\n",
      "   1,stranded     0.0000    0.0000    0.0000         0\n",
      "      4,after     0.0000    0.0000    0.0000         0\n",
      "      1,earth     0.0000    0.0000    0.0000         0\n",
      "        4,for     0.0000    0.0000    0.0000         0\n",
      "        0,the     0.0000    0.0000    0.0000         0\n",
      "     2,during     0.0000    0.0000    0.0000         0\n",
      "         0,at     0.0000    0.0000    0.0000         1\n",
      "   2,arriving     0.0000    0.0000    0.0000         0\n",
      "    1,staying     0.0000    0.0000    0.0000         0\n",
      "        2,the     0.0000    0.0000    0.0000         0\n",
      "    0,getting     0.0000    0.0000    0.0000         0\n",
      "     1,during     0.0000    0.0000    0.0000         0\n",
      "          2,a     0.0000    0.0000    0.0000         0\n",
      "    1,watched     0.0000    0.0000    0.0000         0\n",
      "      4,death     0.0000    0.0000    0.0000         0\n",
      "     4,during     0.0000    0.0000    0.0000         0\n",
      "     2,having     0.0000    0.0000    0.0000         0\n",
      "   1,tutorial     0.0000    0.0000    0.0000         0\n",
      "     0,heated     0.0000    0.0000    0.0000         0\n",
      "         1,in     0.0000    0.0000    0.0000         0\n",
      "       1,once     0.0000    0.0000    0.0000         0\n",
      "       2,went     0.0000    0.0000    0.0000         0\n",
      "      0,first     0.0000    0.0000    0.0000         0\n",
      "      1,going     0.0000    0.0000    0.0000         0\n",
      "        3,ill     0.0000    0.0000    0.0000         0\n",
      "      1,every     0.0000    0.0000    0.0000         0\n",
      "         1,my     0.0000    0.0000    0.0000         0\n",
      "     0,during     0.0000    0.0000    0.0000         0\n",
      "   0,watching     0.0000    0.0000    0.0000         0\n",
      "       4,this     0.0000    0.0000    0.0000         1\n",
      "        2,boy     0.0000    0.0000    0.0000         0\n",
      "         0,no     0.0000    0.0000    0.0000         0\n",
      "    1,occured     0.0000    0.0000    0.0000         0\n",
      "         1,id     0.0000    0.0000    0.0000         1\n",
      "     0,having     0.0000    0.0000    0.0000         0\n",
      "      0,being     0.0000    0.0000    0.0000         0\n",
      "         2,iv     0.0000    0.0000    0.0000         0\n",
      "  0,realizing     0.0000    0.0000    0.0000         0\n",
      "        1,the     0.0000    0.0000    0.0000         0\n",
      "        5,ill     0.0000    0.0000    0.0000         0\n",
      "         1,on     0.0000    0.0000    0.0000         0\n",
      "    4,hearing     0.0000    0.0000    0.0000         0\n",
      "        2,one     0.0000    0.0000    0.0000         0\n",
      "      2,being     0.0000    0.0000    0.0000         0\n",
      "        4,one     0.0000    0.0000    0.0000         0\n",
      "       2,this     0.0000    0.0000    0.0000         0\n",
      "      0,there     0.0000    0.0000    0.0000         0\n",
      "     1,before     0.0000    0.0000    0.0000         0\n",
      "    1,finding     0.0000    0.0000    0.0000         0\n",
      "        0,one     0.0000    0.0000    0.0000         0\n",
      "         5,id     0.0000    0.0000    0.0000         0\n",
      "         0,as     0.0000    0.0000    0.0000         0\n",
      "   0,insulted     0.0000    0.0000    0.0000         0\n",
      "  0,listening     0.0000    0.0000    0.0000         0\n",
      "         0,is     0.0000    0.0000    0.0000         0\n",
      "        2,day     0.0000    0.0000    0.0000         0\n",
      "       0,four     0.0000    0.0000    0.0000         0\n",
      "        0,out     0.0000    0.0000    0.0000         0\n",
      "       4,lost     0.0000    0.0000    0.0000         0\n",
      "0,discovering     0.0000    0.0000    0.0000         0\n",
      "       2,made     0.0000    0.0000    0.0000         1\n",
      "       0,told     0.0000    0.0000    0.0000         1\n",
      "    2,getting     0.0000    0.0000    0.0000         1\n",
      "   0,whenever     0.0000    0.0000    0.0000         1\n",
      "     2,always     0.0000    0.0000    0.0000         1\n",
      "\n",
      "    micro avg     0.8475    0.8475    0.8475      2000\n",
      "    macro avg     0.0938    0.0780    0.0806      2000\n",
      " weighted avg     0.8357    0.8475    0.8372      2000\n",
      "  samples avg     0.8475    0.8475    0.8475      2000\n",
      "\n",
      "2020-11-06 12:59:21,396 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'dev_loss_history': [1.945324420928955,\n",
       "  1.9326533079147339,\n",
       "  1.559191107749939,\n",
       "  1.3066920042037964,\n",
       "  1.1271493434906006,\n",
       "  1.0238040685653687,\n",
       "  0.9342482089996338,\n",
       "  0.8552535176277161,\n",
       "  0.8065276145935059,\n",
       "  0.739918053150177,\n",
       "  0.703685462474823,\n",
       "  0.6572102904319763,\n",
       "  0.625537097454071,\n",
       "  0.6310291886329651,\n",
       "  0.614797055721283,\n",
       "  0.5761505365371704,\n",
       "  0.549270749092102,\n",
       "  0.5605600476264954,\n",
       "  0.5371593832969666,\n",
       "  0.5069844126701355],\n",
       " 'dev_score_history': [0.3843,\n",
       "  0.3552,\n",
       "  0.521,\n",
       "  0.6278,\n",
       "  0.6784,\n",
       "  0.6954,\n",
       "  0.7255,\n",
       "  0.7495,\n",
       "  0.754,\n",
       "  0.7776,\n",
       "  0.7876,\n",
       "  0.8001,\n",
       "  0.7981,\n",
       "  0.7976,\n",
       "  0.8086,\n",
       "  0.8136,\n",
       "  0.8317,\n",
       "  0.8246,\n",
       "  0.8372,\n",
       "  0.8387],\n",
       " 'test_score': 0.8475,\n",
       " 'train_loss_history': [2.158373929977417,\n",
       "  2.0070091712474825,\n",
       "  1.8302668118476868,\n",
       "  1.6537962676286697,\n",
       "  1.521107925415039,\n",
       "  1.4191316524744033,\n",
       "  1.3354870924949647,\n",
       "  1.2652165032625198,\n",
       "  1.195945608139038,\n",
       "  1.1580571936368942,\n",
       "  1.1074739416837693,\n",
       "  1.0796274485588073,\n",
       "  1.0727766813635826,\n",
       "  1.0386687244176864,\n",
       "  0.9971611914038658,\n",
       "  0.9809861173033714,\n",
       "  0.951008976817131,\n",
       "  0.9351967740654945,\n",
       "  0.9252460431456566,\n",
       "  0.910566789507866]}"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train('model-saves2',\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              anneal_factor=0.5,\n",
    "              patience=8,\n",
    "              max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56JYyBaTkI6T",
    "outputId": "c0a4d09a-d4ca-4f4b-8391-4793243eefbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 06:02:52,135 loading file /content/drive/My Drive/Twitter data/model-saves2/emotion-model.pt\n",
      "[2,i (0.2218)] [0,when (0.0835)]\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "classifier = TextClassifier.load('/content/drive/My Drive/Twitter data/model-saves2/emotion-model.pt')\n",
    "\n",
    "pos_sentence = Sentence(preprocess('I love Python!'))\n",
    "neg_sentence = Sentence(preprocess('Python is the worst!'))\n",
    "\n",
    "classifier.predict(pos_sentence)\n",
    "classifier.predict(neg_sentence)\n",
    "\n",
    "print(pos_sentence.labels, neg_sentence.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-DmTu3u8kI6X",
    "outputId": "9652a5e1-34fd-484e-839d-37fa66714ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your sentence is predicted to portray Joy with 22.17828929424286 % confidence\n"
     ]
    }
   ],
   "source": [
    "# show predictions\n",
    "\n",
    "label_dict = {'0': 'Anger', '1': 'Fear', '2': 'Joy', '3': 'Love', '4': 'Sadness', '5': 'Surprise'}\n",
    "\n",
    "sentence = Sentence(preprocess('I love Python!'))\n",
    "classifier.predict(sentence)\n",
    "\n",
    "if len(sentence.labels) > 0:\n",
    "  print('your sentence is predicted to portray ' + label_dict[sentence.labels[0].value[0]] + ' with', sentence.labels[0].score*100, '% confidence') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a79dw9UFmiAq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Emotion_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
